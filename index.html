<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anilkumar Swamy</title>

    <meta name="author" content="Anilkumar Swamy">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpg" href="images/icon_photo.jpg">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <!-- BIO -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                <!-- <td style="padding:2.5%;width:75%;vertical-align:middle"> -->
                                    <p class="name" style="text-align: center;">
                                        Anilkumar Swamy
                                    </p>
                                    <p style="text-align:center">
                                        Computer Vision Researcher at <a href="https://europe.naverlabs.com">NAVERLABS Europe</a> in Grenoble, France. 
                                        I received my M.Sc. degree from the <a href="https://www.uni-saarland.de/en/home.html">
                                        Computer Science Department</a> at Saarland University in Saarbr端cken, Germany
                                    </p>
                                        <!-- I am a </strong> Researcher</strong> at <a href="https://europe.naverlabs.com">NAVERLABS Europe</a> in Grenoble, France. 
                                        I received my M.Sc. degree from the <a href="https://www.uni-saarland.de/en/home.html"> 
                                            Computer Science Department</a> at Saarland University in Saarbr端cken, Germany 
                                        and Inria Grenoble (<a href="https://team.inria.fr/thoth">the THOTH Team</a>) in France,
                                        under the supervision of
                                        <a href="https://www.skamalas.com">Yannis Kalantidis</a>, 
                                        <a href="https://dlarlus.github.io">Diane Larlus</a> and
                                        <a href="https://thoth.inrialpes.fr/people/alahari">Karteek Alahari</a>. 
                                        My PhD focused on learning general-purpose visual representations from images.
                                    <p>
                                        I received my M.Sc. degree from <a href="https://www.uni-saarland.de/en/home.html">the 
                                            Computer Science Department</a> at Saarland University in Saarbr端cken, Germany 
                                             where I worked with <a
                                            href="https://user.ceng.metu.edu.tr/~gcinbis">Gokberk Cinbis</a> on learning
                                        data-efficient visual classification models.
                                        Before that, I received my B.Sc. from <a href="https://eem.eskisehir.edu.tr">the Electrical
                                            and Electronics Engineering Department</a> at Anadolu (now Eskisehir
                                        Technical) University in T端rkiye. 
                                    </p> -->
                                    <p 
                                        style="text-align:center">
                                        <a href="mailto:aniles.ec@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/********.pdf">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=l55w7fgAAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/aniles_ec">Twitter</a> &nbsp/&nbsp
                                        <a href="https://github.com/anilesec">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:25%;max-width:25%">
                                    <img style="max-width:100%;height:auto;" alt="profile photo"
                                        src="images/profile_photo.jpg">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- News-->
                    <!-- <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                    <ul>
                                        <li>
                                            [<em>2023-07</em>] -
                                            Joined <a href="https://europe.naverlabs.com/"> NAVER LABS Europe</a> as a research scientist!
                                        </li>
                                        <li>
                                            [<em>2023-06</em>] -
                                            Successfully defended my PhD thesis!
                                        </li>
                                        <li>
                                            [<em>2023-02</em>] -
                                            <a href="#row-22sd">ImageNet-SD</a> is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!<br>
                                        </li>
                                        <li>
                                            [<em>2023-01</em>] -
                                            <a href="#row-22trex">t-ReX</a> is accepted to <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">ICLR 2023</a> as a spotlight presentation!<br>
                                        </li>
                                        <li>
                                            [<em>2022-12</em>] -
                                            New pre-print on arXiv: <a href="https://arxiv.org/abs/2212.08420">Fake it till you make it: Learning(s) from a synthetic ImageNet clone</a>
                                        </li>
                                        <li>
                                            [<em>2022-10</em>] -
                                            I'm selected as an <a href="https://eccv2022.ecva.net/program/outstanding-reviewers">outstanding
                                                reviewer at ECCV 2022</a>.
                                        </li>
                                        <li>
                                            [<em>2022-06</em>] -
                                            Our work <a href="#row-22trex">Improving the Generalization of
                                                Supervised Models</a> is on <a
                                                href="https://arxiv.org/abs/2206.15369">ArXiv</a>.<br>
                                            Pretrained weights for t-ReX and t-ReX* are available on <a
                                                href="https://europe.naverlabs.com/research/computer-vision/improving-the-generalization-of-supervised-models/#pretrained_models">our
                                                project
                                                webpage</a>.
                                        </li>
                                        <li>
                                            [<em>2021-08</em>] -
                                            <a href="#row-21cog">The ImageNet-CoG benchmark</a> is accepted to <a
                                                href="http://iccv2021.thecvf.com/home">ICCV 2021</a>!<br>
                                            We have a newer version of the manuscript, and our code is out!
                                        </li>
                                        <li>
                                            [<em>2021-07</em>] -
                                            I'm selected as an <a href="http://cvpr2021.thecvf.com/node/184">outstanding
                                                reviewer at CVPR 2021</a>.
                                        </li>
                                        <li>
                                            [<em>2020-12</em>] -
                                            The pre-print of the ImageNet-CoG Benchmark is on <a
                                                href="https://arxiv.org/abs/2012.05649">arXiv</a>.
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            I officially started my PhD at <a
                                                href="https://www.univ-grenoble-alpes.fr">University of Grenoble</a>!
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            <a href="#row-20mochi">MoCHi</a> is accepted to <a
                                                href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>!
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20icmlm">ICMLM</a> is accepted to <a
                                                href="https://eccv2020.eu">ECCV 2020</a>!
                                            Check out our demo <a
                                                href="https://icmlm.europe.naverlabs.com/public">here</a> (it is very
                                            cool!).
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20keyp">"Key protected classification for collaborative
                                                learning"</a> is accepted to <a
                                                href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320301308">Pattern
                                                Recognition</a>!
                                        </li>
                                        <li> [<em>2019-09</em>] - I Joined NAVER LABS Europe as a researcher. </li>
                                        <li> [<em>2019-09</em>] - I defended my master thesis, yey! </li>
                                        <li>
                                            [<em>2019-05</em>] -
                                            <a href="#row-19gmn">GMN</a> is accepted to <a
                                                href="https://cvpr2019.thecvf.com/">CVPR 2019</a> (oral presentation)!
                                        </li>
                                    </ul>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <!-- Research -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <!-- <h3>Research </h3> -->
                                    <!-- <heading>Research</heading> -->
                                    <p>
                                        <!-- I'm broadly interested in computer vision problems. -->
                                        <!-- My main research interests lie in 2D and 3D computer vision and, machine learning. -->
                                        <!-- These include topics such as <b> hand pose/shape estimation, human shape/pose estimation,
                                        and modelling the interaction of human-objects and human-scene.</b>
                                        I am also interested in solving egocentric and monocular computer vision tasks. -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Publications -->
                    <table
                        style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <h3>Publications </h3> -->
                        <heading>Publications</heading>
                        <tbody>
                            <tr id="row-22trex" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="images/2023/showme_teaser.png" target="_blank">
                                    <!-- <img id="myImg" src="images/2023/showme_teaser.png"
                                        alt="SHOWMe Dataset Teaser"
                                        style="width:100%;height:auto"> -->

                                        <video width="100%" height="auto" controls autoplay muted>
                                            <source src="videos/DatasetOverview.mp4" type="video/mp4">
                                            Your browser does not support the video tag.
                                        </video>                                        
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="">
                                        <papertitle>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</papertitle>
                                    </a>
                                    <br>
                                    <b>Anilkumar Swamy</b>, Vincent Leroy, Philippe Weinzaepfel, Fabien Baradel,
                                    Salma Galaaoui, Romain Bregier, Matthieu Armando, Jean-Sebastien Franco, Gregory Rogez
                                    <br>
                                    <!-- <em><a href="https://iplab.dmi.unict.it/acvr2023/"> ICCV/ACVR 2023 (Oral) </a></em> -->
                                    <em><a href="https://iplab.dmi.unict.it/acvr2023/"> ICCV/ACVR 2023 <span style="color: red;">Oral</span> </a></em>
                                    <br>
                                    <p>
                                        In this work, we introduce a high quality hand-object dataset
                                         with 3D pose, shape, texture annotations and parametric models.
                                         We also devise a pipeline for category-agnostic 3D hand-object reconstruction baselines.<br>
                                        <!-- <a href="https://kinovis.inria.fr/4dhumanoutfit">project website</a>,
                                        <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">OpenReview</a> -->
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-22trex" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="images/2022/teaser.png" target="_blank">
                                    <img id="myImg" src="images/2022/teaser.png"
                                        alt="4DHumanOutfit Dataset Teaser"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/pdf/2306.07399.pdf">
                                        <papertitle> 4DHumanOutfitt: a multi-subject 4D dataset of human motion
                                            sequences in varying outfits exhibiting large displacements</papertitle>
                                    </a>
                                    <br>
                                    Matthieu Armando, Laurence Boissieux, Edmond Boyer, Jean-Sebastien, Franco Martin Humenberger, Christophe Legras,
                                    Vincent Leroy, Mathieu Marsot, Julien Pansiot, Sergi Pujades, Rim Rekik, Gregory Rogez,
                                     <b>Anilkumar Swamy</b>, Stefanie Wuhrer (<i style="color:blue;">Authors listed in alphabetuical order</i>)
                                    <br>
                                    <em><a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314223002163?dgcid=rss_sd_all"> CVIU 2023</a></em>
                                    <br>
                                    <p>
                                        4DHumanOutfit is a new dataset of densely sampled spatio-temporal 4D human motion
                                         data of different actors, outfits and motions. The dataset is designed to contain different actors
                                          wearing different outfits each while and performing different motions in each outfit.
                                           In this way, the dataset can be seen as a cube of data containing 4D motion sequences
                                           along the axes identity, outfit, and motion.<br>
                                        <!-- <a href="https://europe.naverlabs.com/t-rex">project website</a>,
                                        <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">OpenReview</a> -->
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-21cog" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="images/2020-EVBP/MILP_Solver.png" target="_blank">
                                    <img id="myImg" src="images/2020-EVBP/MILP_Solver.png"
                                        alt="MILP_Solver"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://ieeexplore.ieee.org/abstract/document/9320065">
                                        <papertitle>Electric Vehicle User Behavior Prediction using Learning-based Approaches</papertitle>
                                    </a>
                                    <br>
                                    Sara Khan, Boris Brandherm, <b>Anilkumar Swamy</b>
                                    <br>
                                    <em><a href="https://epec2020.ieee.ca/">IEEE Electric Power and Energy Conference (EPEC 2020)</a></em>
                                    <br>
                                    <p>
                                        We employ machine learning (ML) and deep learning (DL) methodologies to forecast the behaviors of electric vehicle (EV) users.
                                        By analyzing and contrasting these outcomes, we derive insights to comprehend variations in performance efficacy.<br>
                                        <!-- <a href="https://europe.naverlabs.com/cog-benchmark">project website</a>,
                                        <a href="https://github.com/naver/cog">code</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_poster.pdf">poster</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pdf">presentation
                                            (PDF)</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pptx">presentation
                                            (PPT)</a>,
                                        <a href="https://youtu.be/1pMkHaETA6U">video</a> -->
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-22trex" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="images/2019-FDD/DefectDetectOverviewFigure.png" target="_blank">
                                    <img id="myImg" src="images/2019-FDD/DefectDetectOverviewFigure.png"
                                        alt="Pipeline to mine defects in clothes"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1212&context=wi2019">
                                        <papertitle> Leveraging Unstructured Image Data for Product Quality Improvement</papertitle>
                                    </a>
                                    <br>
                                    Oliver Nalbach, Maximilian Derouet, <b>Anilkumar Swamy</b>, Dirk Werth
                                    <br>
                                    <em><a href="https://wi2019.de">14th International Conference on Wirtschaftsinformatik 2019</a></em>
                                    <br>
                                    <p>
                                        we discuss the potential of leveraging initially unstructured
                                        information in the form of images, taken either during quality checks or by
                                        customers when returning a product, to the end of product quality improvement.
                                        We furthermore show how this might be realized in practice using the case of
                                        fashion manufacturing as an example.<br>
                                        <!-- <a href="https://europe.naverlabs.com/t-rex">project website</a>,
                                        <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">OpenReview</a> -->
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr id="row-20mochi" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2020-mochi/mochi-mixing-negatives.png" target="_blank">
                                    <img id="myImg" src="pub/2020-mochi/mochi-mixing-negatives.png"
                                        alt="Mixing negatives in MoCHi"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2010.01028">
                                        <papertitle>MoCHi | Hard Negative Mixing for Contrastive Learning</papertitle>
                                    </a>
                                    <br>
                                    Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel and Diane Larlus
                                    <br>
                                    <em><a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a></em>
                                    <p>
                                        For contrastive learning, sampling more or harder negatives often improve
                                        performance.
                                        We propose two ways to synthesize more negatives using the MoCo framework.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/mochi">project
                                            website</a>
                                    </p>
                                </td>
                            </tr> -->

                            <!-- <tr id="row-20icmlm" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2020-icmlm/masked-token-attention.gif" target="_blank">
                                    <img id="myImg" src="pub/2020-icmlm/masked-token-attention.gif"
                                        alt="Masked token attention in ICMLM"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2008.01392">
                                        <papertitle>ICMLM | Learning Visual Representations with Caption Annotations
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Julien Perez and Diane Larlus
                                    <br>
                                    <em><a href="https://eccv2020.eu/">ECCV 2020</a></em>
                                    <p>
                                        Images often come with accompanying text describing the scene in images.
                                        We propose a method to learn visual representations using (image, caption)
                                        pairs.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/icmlm/">project
                                            website</a>,
                                        <a href="https://icmlm.europe.naverlabs.com/public">demo</a>
                                    </p>
                                </td>
                            </tr> -->

                            <!-- <tr id="row-20keyp" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2020-keyp/keyp-compute-chain.png" alt="keyp-compute-chain"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/1908.10172">
                                        <papertitle>Key protected classification for collaborative learning</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Ramazan Gokberk Cinbis and Erman Ayday
                                    <br>
                                    <em><a
                                            href="https://www.sciencedirect.com/journal/pattern-recognition/vol/104/suppl/C">Pattern
                                            Recognition, Vol. 104, August 2020</a></em>
                                    <p>
                                        Vanilla collaborative learning frameworks are vulnerable to an active adversary
                                        that runs a generative adversarial network attack.
                                        We propose a classification model that is resilient against such attacks by
                                        design.<br>
                                        <a href="https://github.com/mbsariyildiz/key-protected-classification">code
                                            repo</a>
                                    </p>
                                </td>
                            </tr> -->

                            <!-- <tr id="row-19gmn" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2019-gmn/gmn-front.png" alt="gmn-front-figure"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a
                                        href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sariyildiz_Gradient_Matching_Generative_Networks_for_Zero-Shot_Learning_CVPR_2019_paper.html">
                                        <papertitle>GMN | Gradient Matching Generative Networks for Zero-Shot Learning
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz and Ramazan Gokberk Cinbis
                                    <br>
                                    <em><a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>, oral presentation</em>
                                    <p>
                                        Zero-shot learning models may suffer from the domain-shift due to the difference
                                        between data distributions of seen and unseen concepts.
                                        We propose a generative model to synthesize samples for unseen concepts given
                                        their visual attributes and use these samples for training a classifier for both
                                        seen and unseen concepts.<br>
                                        <a href="https://github.com/mbsariyildiz/gmn-zsl">code repo</a>
                                    </p>
                                </td>
                            </tr> -->

                        </tbody>
                    </table>

                    <!-- Community service -->
                    <!-- <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Community Service</heading>
                                    <p>

                                    <ul>
                                        <li><a href="">Reviewer for ECCV 2022</a></li>
                                        <li><a href="https://cvpr2022.thecvf.com/all-reviewers">Reviewer for CVPR
                                                2022</a></li>
                                        <li><a href="">Reviewer for ICCV 2021</a></li>
                                        <li><a href="http://cvpr2021.thecvf.com/node/181">Reviewer for CVPR 2021</a>
                                        </li>
                                        <li><a href="https://nips.cc/Conferences/2020/Reviewers">Reviewer for NeurIPS
                                                2020</a></li>
                                    </ul>

                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <!-- Website notice -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <p style="text-align:right;font-size:small;">
                                        website template <a href="https://jonbarron.info/">source code</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>